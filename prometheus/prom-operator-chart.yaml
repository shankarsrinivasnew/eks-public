## helm upgrade --install <env> prometheus-community/kube-prometheus-stack -f prom-operator-chart.yaml -f prometheus-rules.yaml
namespaceOverride: "prometheus"

## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
##
grafana:
  enabled: true
  envFromSecret: grafana-mysql-credentials ## Configure RDS Backend. Ensure Secret named grafana-mysql-credentials is created prior to runninng this.
  # env:
  #   http_proxy: http://eu-west-2.proxy.aws.uk.tsb:3128
  #   https_proxy: http://eu-west-2.proxy.aws.uk.tsb:3128
  #   no_proxy: localhost,127.0.0.1,169.254.169.254,10.0.0.0/8,172.20.0.0/16,.internal,.eu-west-2.elb.amazonaws.com,vault.service.consul,ssm.eu-west-2.amazonaws.com,ssmmessages.eu-west-2.amazonaws.com,ec2.eu-west-2.amazonaws.com,ec2messages.eu-west-2.amazonaws.com,s3.eu-west-2.amazonaws.com,autoscaling.eu-west-2.amazonaws.com,secretsmanager.eu-west-2.amazonaws.com,kms.eu-west-2.amazonaws.com,ecr.eu-west-2.amazonaws.com,.eks.amazonaws.com,.ecr.eu-west-2.amazonaws.com,.s3.eu-west-2.amazonaws.com,.cluster.local,.aws.uk.tsb
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internal
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0
      alb.ingress.kubernetes.io/success-codes: 200,301,302
      external-dns.alpha.kubernetes.io/hostname: grafana.prod.aws.uk.tsb
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:eu-west-2:407929326957:certificate/6b3b2442-cb1d-4b12-8d93-696603630bbe
    path: /*
  sidecar:
    datasources:
      defaultDatasourceEnabled: false
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Thanos
        type: prometheus
        url: http://thanos-query-http.prometheus:10902 #Thanos must be deployed first for this to work.
        access: proxy
        isDefault: true
  # plugins:
  #   - "grafana-piechart-panel"
  #   - "camptocamp-prometheus-alertmanager-datasource"
  #   - "praj-ams-datasource"

prometheusOperator:
  enabled: true
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 50m
      memory: 50Mi

  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
  ##
  hostNetwork: false

## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: false
## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false
## Component scraping kube proxy
##
kubeProxy:
  enabled: false

## Deploy a Prometheus instance
##
prometheus:
  enabled: true
  serviceAccount:
    create: true
    name: prometheus-sa
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::407929326957:role/eks/prometheus-thanos-s3
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internal
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0
      alb.ingress.kubernetes.io/success-codes: 200,301,302
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
      external-dns.alpha.kubernetes.io/hostname: prometheus.prod.aws.uk.tsb
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:eu-west-2:407929326957:certificate/44ce6501-b708-4420-b4f9-9cc2961697aa
    paths:
      - /*

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:
    containers:
      - name: prometheus
        env:
          - name: http_proxy
            value: http://eu-west-2.proxy.aws.uk.tsb:3128
          - name: https_proxy
            value: http://eu-west-2.proxy.aws.uk.tsb:3128
          - name: no_proxy
            value: localhost,127.0.0.1,169.254.169.254,10.0.0.0/8,172.20.0.0/16,.internal,.eu-west-2.elb.amazonaws.com,vault.service.consul,ssm.eu-west-2.amazonaws.com,ssmmessages.eu-west-2.amazonaws.com,ec2.eu-west-2.amazonaws.com,ec2messages.eu-west-2.amazonaws.com,s3.eu-west-2.amazonaws.com,autoscaling.eu-west-2.amazonaws.com,secretsmanager.eu-west-2.amazonaws.com,kms.eu-west-2.amazonaws.com,ecr.eu-west-2.amazonaws.com,.eks.amazonaws.com,.ecr.eu-west-2.amazonaws.com,.s3.eu-west-2.amazonaws.com,.cluster.local,.aws.uk.tsb
    podMetadata:
      annotations:
        iam.amazonaws.com/role: arn:aws:iam::407929326957:role/eks/prometheus-thanos-s3
    retention: 6h
    disableCompaction: true
    storageSpec:
      volumeClaimTemplate:
        spec:
          volumeName: prometheus-data #ENSURE PV created before running this ref: storage.yaml
          accessModes:
            - ReadWriteMany
          storageClassName: efs-sc
          resources:
            requests:
              storage: 20Gi
          selector:
            matchLabels:
              prometheus: "data"
    additionalScrapeConfigs:
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090
      - job_name: alertmanager
        static_configs:
          - targets:
            - localhost:9093
      - job_name: ec2-dynamic-discovery
        ec2_sd_configs:
          - region: eu-west-2
            port: 9100
        relabel_configs:
          - source_labels: [__meta_ec2_private_ip]
            target_label: privateip
          - source_labels: [__meta_ec2_instance_id]
            target_label: instanceid
          - source_labels: [__meta_ec2_tag_ManagedBy]
            target_label: managedby
          - source_labels: [__meta_ec2_tag_Name]
            target_label: instance
          - source_labels: [__meta_ec2_tag_Environment]
            target_label: environment
          - source_labels: [__meta_ec2_availability_zone]
            target_label: availabilityzone
          - source_labels: [__meta_ec2_instance_state]
            target_label: status
          - source_labels: [__meta_ec2_instance_type]
            target_label: instancetype
          - source_labels: [__meta_ec2_tag_subsystem]
            target_label: subsystem
      - job_name: jmx-hdf-exporter
        static_configs:
          - targets:
            - controller.hdf.prod.aws.uk.tsb:9110
            - service-0.hdf.prod.aws.uk.tsb:9110
            - service-1.hdf.prod.aws.uk.tsb:9110
            - service-2.hdf.prod.aws.uk.tsb:9110
            - kafka-0.hdf.prod.aws.uk.tsb:9110
            - kafka-1.hdf.prod.aws.uk.tsb:9110
            - kafka-2.hdf.prod.aws.uk.tsb:9110
            - kafka-3.hdf.prod.aws.uk.tsb:9110
            - kafka-4.hdf.prod.aws.uk.tsb:9110
            - kafka-5.hdf.prod.aws.uk.tsb:9110
      - job_name: hdf-controller-node-exporter
        static_configs:
          - targets:
            - controller.hdf.prod.aws.uk.tsb:9100
      - job_name: hdf-service-node-exporter
        static_configs:
          - targets:
            - service-0.hdf.prod.aws.uk.tsb:9100
            - service-1.hdf.prod.aws.uk.tsb:9100
            - service-2.hdf.prod.aws.uk.tsb:9100
      - job_name: kakfa-node-exporter
        static_configs:
          - targets:
            - kafka-0.hdf.prod.aws.uk.tsb:9100
            - kafka-1.hdf.prod.aws.uk.tsb:9100
            - kafka-2.hdf.prod.aws.uk.tsb:9100
            - kafka-3.hdf.prod.aws.uk.tsb:9100
            - kafka-4.hdf.prod.aws.uk.tsb:9100
            - kafka-5.hdf.prod.aws.uk.tsb:9100
      - job_name: scylla-node-exporter
        static_configs:
          - targets:
            - ip-10-163-148-232.eu-west-2.compute.internal:9100
            - ip-10-163-155-7.eu-west-2.compute.internal:9100
            - ip-10-163-144-194.eu-west-2.compute.internal:9100
      - job_name: scylla-default-metrics-exporter
        static_configs:
          - targets:
            - ip-10-163-148-232.eu-west-2.compute.internal:9180
            - ip-10-163-155-7.eu-west-2.compute.internal:9180
            - ip-10-163-144-194.eu-west-2.compute.internal:9180
      - job_name: consul-node-exporter
        static_configs:
          - targets:
            - ip-10-163-236-45.eu-west-2.compute.internal:9100
            - ip-10-163-164-21.eu-west-2.compute.internal:9100
            - ip-10-163-203-221.eu-west-2.compute.internal:9100
            - ip-10-163-227-130.eu-west-2.compute.internal:9100
            - ip-10-163-191-88.eu-west-2.compute.internal:9100
            - ip-10-163-211-243.eu-west-2.compute.internal:9100

    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    serviceMonitorSelectorNilUsesHelmValues: false
    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the PrometheusRule resources created
    ##
    ruleSelectorNilUsesHelmValues: false
    #ruleSelector:
    #   matchLabels:
    #     app: kube-prometheus-stack
    #     release: prod

    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
    ## This section is experimental, it may change significantly without deprecation notice in any release.
    ## This is experimental and may change significantly without backward compatibility in any release.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec
    ##
    thanos:
      image: quay.io/thanos/thanos:v0.15.0
      version: v0.15.0
      objectStorageConfig:
        name: thanos-s3-config
        key: object-store.yaml

## Configuration for alertmanager
##
alertmanager:
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'dev', 'instance']
    receivers:
      - name: "null"
      - name: "sns-forwarder-cbalerts"
        webhook_configs:
        - url: "http://alertmanager-sns-forwarder-svc:9087/alert/CB-alerts"
          send_resolved: true
      - name: 'cb-slack-notifications'
        slack_configs:
        - channel: '#cb_alert_notifications'
          api_url: 'https://hooks.slack.com/services/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'
          icon_url: 'https://github.com/prometheus/docs/raw/master/static/prometheus_logo.png'
          title: |-
            [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
            {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
              {{" "}}(
              {{- with .CommonLabels.Remove .GroupLabels.Names }}
                {{- range $index, $label := .SortedPairs -}}
                  {{ if $index }}, {{ end }}
                  {{- $label.Name }}="{{ $label.Value -}}"
                {{- end }}
              {{- end -}}
              )
            {{- end }}
          text: >-
            {{ range .Alerts -}}
            *Alert:* {{ .Annotations.summary }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}

            *Description:* {{ .Annotations.description }}

            *Details:*
              {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
              {{ end }}
            {{ end }}
          send_resolved: true
          http_config:
            proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
    route:
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 1m
      receiver: "null"
      routes:
        - match:
            namespace: cb
          receiver: sns-forwarder-cbalerts
          continue: true
        - match:
            namespace: cb
          receiver: cb-slack-notifications
          continue: true
