## helm upgrade --install <env> prometheus-community/kube-prometheus-stack -f prom-operator-chart.yaml
namespaceOverride: "prometheus"

## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
##
grafana:
  enabled: true
  envFromSecret: grafana-mysql-credentials ## Configure RDS Backend. Ensure Secret named grafana-mysql-credentials is created prior to runninng this.
  env:
    http_proxy: http://eu-west-2.proxy.aws.uk.tsb:3128
    https_proxy: http://eu-west-2.proxy.aws.uk.tsb:3128
    no_proxy: localhost,127.0.0.1,169.254.169.254,10.0.0.0/8,172.20.0.0/16,.internal,.eu-west-2.elb.amazonaws.com,vault.service.consul,ssm.eu-west-2.amazonaws.com,ssmmessages.eu-west-2.amazonaws.com,ec2.eu-west-2.amazonaws.com,ec2messages.eu-west-2.amazonaws.com,s3.eu-west-2.amazonaws.com,autoscaling.eu-west-2.amazonaws.com,secretsmanager.eu-west-2.amazonaws.com,kms.eu-west-2.amazonaws.com,ecr.eu-west-2.amazonaws.com,.eks.amazonaws.com,.ecr.eu-west-2.amazonaws.com,.s3.eu-west-2.amazonaws.com,.cluster.local,.aws.uk.tsb
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internal
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0
      alb.ingress.kubernetes.io/success-codes: 200,301,302
      external-dns.alpha.kubernetes.io/hostname: grafana.prod.aws.uk.tsb
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:eu-west-2:407929326957:certificate/6b3b2442-cb1d-4b12-8d93-696603630bbe
    path: /*
  sidecar:
    datasources:
      defaultDatasourceEnabled: false
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Thanos
        type: prometheus
        url: http://thanos-query-http.prometheus:10902 #Thanos must be deployed first for this to work.
        access: proxy
        isDefault: true
  plugins: 
    - "grafana-piechart-panel"

prometheusOperator:
  enabled: true
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 50m
      memory: 50Mi

  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
  ##
  hostNetwork: false

## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: false
## Component scraping kube scheduler
##
kubeScheduler:
  enabled: false
## Component scraping kube proxy
##
kubeProxy:
  enabled: false

## Deploy a Prometheus instance
##
prometheus:
  enabled: true
  serviceAccount: 
    create: true
    name: prometheus-sa
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::273312704578:role/eks/prometheus-thanos-s3
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internal
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0/0
      alb.ingress.kubernetes.io/success-codes: 200,301,302
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
      external-dns.alpha.kubernetes.io/hostname: prometheus.prod.aws.uk.tsb
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:eu-west-2:407929326957:certificate/44ce6501-b708-4420-b4f9-9cc2961697aa
    paths:
      - /*

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:
    additionalScrapeConfigs:
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090
      - job_name: kakfa-node-exporter
        static_configs:
          - targets:
            - kafka-0.hdf.prod.aws.uk.tsb:9100
            - kafka-1.hdf.prod.aws.uk.tsb:9100
            - kafka-2.hdf.prod.aws.uk.tsb:9100
            - kafka-3.hdf.prod.aws.uk.tsb:9100
            - kafka-4.hdf.prod.aws.uk.tsb:9100
            - kafka-5.hdf.prod.aws.uk.tsb:9100
      - job_name: scylla-node-exporter
        static_configs:
          - targets:
            - ip-10-163-148-232.eu-west-2.compute.internal:9180
            - ip-10-163-155-7.eu-west-2.compute.internal:9180
            - ip-10-163-144-194.eu-west-2.compute.internal:9180
      
      # - job_name: blackbox-exporter-prod
      #   scrape_interval: 1m
      #   metrics_path: /probe
      #   params:
      #     module: [http_2xx]
      #   static_configs:
      #     - targets:
      #       - ranger.hdf.prod.aws.uk.tsb:6182
      #       - ranger-00.hdf.prod.aws.uk.tsb:6182
      #       - ranger-01.hdf.prod.aws.uk.tsb:6182
      #       - ranger-02.hdf.prod.aws.uk.tsb:6182
      #       - ambari.hdf.prod.aws.uk.tsb:8443
      # - job_name: 'Scylla Exporter - PROD'
      #   proxy_url: http://eu-west-2.proxy.aws.uk.tsb:3128
      #   ec2_sd_configs:
      #     - region: eu-west-2
      #       role_arn: arn:aws:iam::407929326957:role/eks/prometheus-thanos-s3
      #       port: 9180
      #   relabel_configs:
      #       # Use the instance ID as the instance label
      #     - source_labels: [__meta_ec2_private_ip]
      #       target_label: privateip
      #     - source_labels: [__meta_ec2_instance_id]
      #       target_label: instanceid
      #     - source_labels: [__meta_ec2_tag_description]
      #       target_label: description
      #     - source_labels: [__meta_ec2_tag_Name]
      #       target_label: instance
      #     - source_labels: [__meta_ec2_tag_environment]
      #       target_label: environment
      #     - source_labels: [__meta_ec2_tag_project]
      #       target_label: project
      #     - source_labels: [__meta_ec2_instance_state]
      #       target_label: status
      #     - source_labels: [__meta_ec2_tag_subsystem]
      #       target_label: subsystem
      #       regex: scylladb
      #       action: keep
      #     - source_labels: [__meta_ec2_tag_subsystem]
      #       target_label: subsystem

    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    serviceMonitorSelectorNilUsesHelmValues: false
    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
    ## This section is experimental, it may change significantly without deprecation notice in any release.
    ## This is experimental and may change significantly without backward compatibility in any release.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec
    ##
    thanos:
      image: quay.io/thanos/thanos:v0.15.0
      version: v0.15.0
      objectStorageConfig:
        name: thanos-s3-config
        key: object-store.yaml