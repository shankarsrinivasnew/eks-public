apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: kube-prometheus-stack
    release: prometheus
  name: rax-cw-middleware-monitoring
  namespace: prometheus
spec:
  groups:
    - name: MIDDLEWARE_MONITORING_RULES
      rules:
        - alert: scylla_db_node_count
          expr: count(up{job="ec2-discover-node-exporter",instance=~".*scylla.*"} == 1) < 3
          for: 15m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: middleware
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            summary: Scylla in {{ $labels.environment }} has less than 3 nodes.

        # - alert: kafka_broker_count
        #   expr: count(kafka_server_replicamanager_leadercount{job="ec2-discover-hdf-jmx"}) < 3
        #   for: 15m
        #   labels:
        #     severity: CRITICAL
        #     priority: P4
        #     environment: "{{ AWS_ENVIRONMENT }}"
        #     monitoring: middleware
        #     rackspace: "true"
        #     hostname: "{{ $labels.instance }}"
        #     tsbci: AWS_ELZ
        #   annotations:
        #     summary: Critical Alert{{ $labels.job }}, Kafka has been running on less than 3 brokers for the last 15mins.

        # - alert: kafka_failed_fetch_request_rate
        #   expr: sum(kafka_server_brokertopicmetrics_meanrate{job="ec2-discover-hdf-jmx",name="FailedFetchRequestsPerSec",topic!=""})by (topic) * 100 > 5
        #   for: 15m
        #   labels:
        #     severity: HIGH
        #     environment: "{{ AWS_ENVIRONMENT }}"
        #     monitoring: middleware
        #     rackspace: "true"
        #   annotations:
        #     summary: Critical Alert, Application {{ $labels.job }} failed request rate is greater than 5%

        - alert: jvm_heap_size
          expr: (avg(jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"}) by (instance)) * 100 > 95
          for: 15m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: middleware
            rackspace: "true"
            hostname: "{{ $labels.pod }}"
            tsbci: AWS_ELZ
          annotations:
            summary: Application {{ $labels.service }} in {{ $labels.environment }} jvm heap size is over 95% for 15mins

        - alert: consul_up
          expr: consul_up < 1
          for: 15m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: middleware
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            summary: Application {{ $labels.service_name }} in {{ $labels.environment }} is down

        - alert: members_in_cluster
          expr: consul_serf_lan_members < 6 
          for: 15m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: middleware
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            summary: Application {{ $labels.service_name }} in {{ $labels.environment }} has less than 6 nodes

        - alert: unhealthy_consul_service
          expr: (consul_health_service_status{status="passing"}) < 1
          for: 15m
          labels:
            severity: WARNING
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: middleware
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            summary: consul service {{ $labels.service_name }} in {{ $labels.environment }} is unhealthy

        - alert: unhealthy_node
          expr: consul_health_node_status{status!='passing'} > 0
          for: 15m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: middleware
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            summary: Consul node {{ $labels.node }} in {{ $labels.environment }} is unhealthy