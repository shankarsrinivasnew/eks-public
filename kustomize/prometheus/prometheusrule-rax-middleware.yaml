apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: kube-prometheus-stack
    release: prometheus
  name: rax-cw-middleware-monitoring
  namespace: prometheus
spec:
  groups:
  - name: MIDDLEWARE_MONITORING_RULES
    rules:
    - alert: scylla_db_node_count
      expr: count(up{job="ec2-discover-node-exporter",instance=~".*scylla.*"}) < 3
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: Application {{ $labels.instance }} has less than 3 nodes.

    # - alert: kafka_broker_count
    #   expr: count(kafka_server_replicamanager_leadercount{job="ec2-discover-hdf-jmx"}) < 3
    #   for: 15m
    #   labels:
    #     severity: HIGH
    #     environment: preprod
    #     monitoring: middleware
    #     rackspace: "true"
    #   annotations:
    #     summary: Critical Alert, Application {{ $labels.job }} has been running on less than 3 brokers for the last 15mins.

    # - alert: kafka_failed_fetch_request_rate
    #   expr: sum(kafka_server_brokertopicmetrics_meanrate{job="ec2-discover-hdf-jmx",name="FailedFetchRequestsPerSec",topic!=""})by (topic) / 10 > 5
    #   for: 15m
    #   labels:
    #     severity: HIGH
    #     environment: "{{ AWS_ENVIRONMENT }}"
    #     monitoring: middleware
    #     rackspace: "true"
    #   annotations:
    #     summary: Critical Alert, Application {{ $labels.job }} failed request rate is greater than 5%
    
    - alert: jvm_heap_size
      expr: (jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"}) * 100 > 95
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: Application {{ $labels.service }} jvm heap size is over 95%

    - alert: consul_up
      expr: consul_up < 1
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: Application {{ $labels.service_name }} is down
        
    - alert: consul_raft_leader
      expr: consul_raft_leader < 1
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: Application {{ $labels.service_name }} raft leader is down

    - alert: members_in_cluster
      expr: consul_serf_lan_members < 6 
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: Application {{ $labels.service_name }} has less than 6 nodes

    - alert: unhealthy_consul_service
      expr: sum by (service_name, status, node) (consul_health_service_status{status!="passing"}) > 0
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: consul service {{ $labels.service_name }} is unhealthy

    - alert: unhealthy_node
      expr: consul_health_node_status{status!='passing'} > 0
      for: 15m
      labels:
        severity: HIGH
        environment: "{{ AWS_ENVIRONMENT }}"
        monitoring: middleware
        rackspace: "true"
      annotations:
        summary: Consul node {{ $labels.node }} is unhealthy
