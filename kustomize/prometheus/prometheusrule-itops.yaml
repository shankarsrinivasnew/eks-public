apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: kube-prometheus-stack
    release: prometheus
  name: itops-alerts
  namespace: prometheus
spec:
  groups:
    - name: KUBE_NODE_ERROR
      rules:
        - alert: KUBE_NODE_ERROR
          expr: (kubelet_node_config_error !=0)
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: node_errors
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Following Kubernetes node has errors {{ $labels.instance }}"
    - name: SCYLLA_READ
      rules:
        - alert: SCYLLA_READ
          expr: (sum(rate(scylla_storage_proxy_coordinator_read_unavailable[60s])) by (cluster, instance) + sum(rate(scylla_storage_proxy_coordinator_write_unavailable[60s])) by (cluster, instance) + sum(rate(scylla_storage_proxy_coordinator_range_unavailable[60s])) by (cluster, instance) != 0 )
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: scylla_read
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Following Scyla instance {{ $labels.instance }} Operation unavailable, read, write operations are unavailable"
    - name: ACTIVE_CONTROLLERS
      rules:
        - alert: ACTIVE_CONTROLLERS
          expr: (sum(rate(scylla_storage_proxy_coordinator_write_unavailable[60s])) by (cluster, instance)!= 0)
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: active_controllers
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "The number of active controllers in {{ $labels.instance }} is not 1. If value is 0 then Kafka is down"
    # - name: KAFKA-ZOOKEEEPER
    #   rules:
    #     - alert: KAFKA-ZOOKEEEPER
    #       expr: probe_success{job="blackbox_zookeeper"} == 0
    #       for: 10m
    #       labels:
    #         severity: CRITICAL
    #         priority: P4
    #         environment: "{{ AWS_ENVIRONMENT }}"
    #         monitoring: kafka-zookeeper
    #         rackspace: "true"
    #         hostname: "{{ $labels.instance }}"
    #         tsbci: AWS_ELZ
    #         itopsbridge: "true"
    #       annotations:
    #         summary: "Zookeeper node {{ $labels.instance }} is failing an endpoint health check"
    - name: LOGSTASH-SERVICE
      rules:
        - alert: LOGSTASH-SERVICE
          expr: (node_systemd_unit_state{instance=~".*logstash.*",name="logstash.service",state="failed"} != 0)
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: logstash-service
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Logstash service failed {{ $labels.instance }}"       
    - name: CONSUL-SERVICE
      rules:
        - alert: CONSUL-SERVICE-AGENT
          expr: (consul_health_node_status{status="critical"} == 1)
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: consul-service
            rackspace: "true"
            hostname: "{{ $labels.node }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            title: Consul agent is down
            description: Consul agent is not healthy on {{ $labels.node }}.
    - name: CONSUL-DATA-CHECK
      rules:
        - alert: CONSUL-DATA-CHECK
          expr: (probe_http_status_code{instance="https://consul.{{ AWS_ENVIRONMENT }}.aws.uk.tsb/v1/kv/config-cb/application/consul/ssl/CACert", job="blackbox_4xx"} != 200)
          for: 10s
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: consul-data-check
            rackspace: "true"
            hostname: "{{ $labels.node }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            title: Consul agent is missing data
            description: Consul data is missing please restore on {{ $labels.node }}.
    - name: CONSUL-SERVICE-LEADER   
      rules:     
        - alert: consul_raft_leader
          expr: consul_raft_leader < 1
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: logstash-service
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: Application {{ $labels.service_name }} in {{ $labels.environment }} raft leader is down
    - name: CONSUL-ENDPOINT
      rules:
        - alert: consul_probe_http_2xx
          expr: (probe_http_status_code{instance="https://consul.{{ AWS_ENVIRONMENT }}.aws.uk.tsb", job="blackbox"}) !=200
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: consul-endpoint
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Consul Endpoint {{ $labels.instance }} unhealthy"
    - name: GRAFANA-ENDPOINT
      rules:
        - alert: grafana_probe_http_2xx
          expr: (probe_http_status_code{instance="https://grafana.{{ AWS_ENVIRONMENT }}.aws.uk.tsb", job="blackbox"}) !=200 and ON() hour() >=18 <=3
          for: 10m
          labels:
            severity: CRITICAL
            app: instance_scheduler
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: grafana-endpoint
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Grafana Endpoint {{ $labels.instance }} unhealthy"
    - name: PROMETHEUS-ENDPOINT
      rules:
        - alert: prometheus_probe_http_2xx
          expr: (probe_http_status_code{instance="https://prometheus.{{ AWS_ENVIRONMENT }}.aws.uk.tsb", job="blackbox"})!=200
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: prometheus-endpoint
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Prometheus Endpoint {{ $labels.instance }} unhealthy"
    - name: VAULT-ENDPOINT
      rules:
        - alert: vault_probe_http_2xx
          expr: (probe_http_status_code{instance="https://vault.{{ AWS_ENVIRONMENT }}.aws.uk.tsb", job="blackbox"})!=200
          for: 10m
          labels:
            severity: CRITICAL
            priority: P4
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: vault-endpoint
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
            itopsbridge: "true"
          annotations:
            summary: "Vault Endpoint {{ $labels.instance }} unhealthy"