---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus
  namespace: prometheus
spec:
  releaseName: prometheus
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: kube-prometheus-stack
    version: 12.7.0
  values:
    ## helm upgrade --install <env> prometheus-community/kube-prometheus-stack -f prom-operator-chart.yaml -f prometheus-rules.yaml
    namespaceOverride: "prometheus"
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: true
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverError: true
        kubeApiserverSlos: true
        kubelet: true
        kubePrometheusGeneral: true
        kubePrometheusNodeAlerting: true
        kubePrometheusNodeRecording: true
        kubernetesAbsent: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        prometheus: true
        prometheusOperator: true
        time: true

    ## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
    ##
    grafana:
      enabled: true
      envFromSecret: grafana-mysql-credentials ## Configure RDS Backend. Ensure Secret named grafana-mysql-credentials is created prior to runninng this.
      # env:
      #   http_proxy: http://eu-west-2.proxy.aws.uk.tsb:3128
      #   https_proxy: http://eu-west-2.proxy.aws.uk.tsb:3128
      #   no_proxy: localhost,127.0.0.1,169.254.169.254,10.0.0.0/8,172.20.0.0/16,.internal,.eu-west-2.elb.amazonaws.com,vault.service.consul,ssm.eu-west-2.amazonaws.com,ssmmessages.eu-west-2.amazonaws.com,ec2.eu-west-2.amazonaws.com,ec2messages.eu-west-2.amazonaws.com,s3.eu-west-2.amazonaws.com,autoscaling.eu-west-2.amazonaws.com,secretsmanager.eu-west-2.amazonaws.com,kms.eu-west-2.amazonaws.com,ecr.eu-west-2.amazonaws.com,.eks.amazonaws.com,.ecr.eu-west-2.amazonaws.com,.s3.eu-west-2.amazonaws.com,.cluster.local,.aws.uk.tsb
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: alb
          alb.ingress.kubernetes.io/scheme: internal
          alb.ingress.kubernetes.io/target-type: ip
          alb.ingress.kubernetes.io/inbound-cidrs: "{{ VPC_CIDR }},10.161.48.0/20,10.198.165.58/32,10.184.128.0/19,10.185.128.0/19,10.186.80.0/22,10.186.84.0/22,172.27.240.0/20"
          alb.ingress.kubernetes.io/success-codes: 200,301,302
          external-dns.alpha.kubernetes.io/hostname: grafana.{{ DNS_ZONE }}
          alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
          alb.ingress.kubernetes.io/backend-protocol: HTTP
        hosts:
          - grafana.{{ DNS_ZONE }}
        path: /*
      sidecar:
        datasources:
          defaultDatasourceEnabled: false
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
          - name: Thanos
            type: prometheus
            url: http://thanos-query-http.prometheus:10902 #Thanos must be deployed first for this to work.
            access: proxy
            isDefault: true
      # plugins:
      #   - "grafana-piechart-panel"
      #   - "camptocamp-prometheus-alertmanager-datasource"
      #   - "praj-ams-datasource"
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
            - name: 'shared-grafana-jvm'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-jvm
            - name: 'shared-grafana-scylla1'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-scylla1
            - name: 'shared-grafana-scylla2'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-scylla2
            - name: 'shared-grafana-kafka'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-kafka
            - name: 'shared-grafana-consul-vault'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-consul-vault
            - name: 'shared-grafana-consul'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-consul
            - name: 'shared-grafana-ec2nodes'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-ec2nodes
            - name: 'shared-grafana-kubepod-apps'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-kubepod-apps
            - name: 'shared-grafana-kubepodoverview'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-kubepodoverview
            - name: 'shared-grafana-kubeprometheus'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-kubeprometheus
            - name: 'shared-grafana-kubestatefulsets'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-kubestatefulsets
            - name: 'shared-grafana-rax-endpoints'
              orgId: 1
              folder: 'Shared Services'
              type: file
              disableDeletion: true
              editable: true
              options:
                path: /var/lib/grafana/dashboards/shared-grafana-rax-endpoints

      dashboardsConfigMaps:
        shared-grafana-jvm: shared-jvm-cm
        shared-grafana-scylla1: shared-scylla1-cm
        shared-grafana-scylla2: shared-scylla2-cm
        shared-grafana-kafka: shared-kafka-cm
        shared-grafana-consul-vault: shared-consul-vault-cm
        shared-grafana-consul: shared-consul-cm
        shared-grafana-ec2nodes: shared-ec2nodes-cm
        shared-grafana-kubepod-apps: shared-kubepod-apps-cm
        shared-grafana-kubepodoverview: shared-kubepodoverview-cm
        shared-grafana-kubeprometheus: shared-kubeprometheus-cm
        shared-grafana-kubestatefulsets: shared-kubestatefulsets-cm
        shared-grafana-rax-endpoints: shared-rax-endpoints-cm



    prometheusOperator:
      enabled: true
      nodeSelector:
        taints.eks.aws.uk.tsb/role: mgmt
      tolerations:
        - key: role
          operator: Equal
          value: mgmt
          effect: NoSchedule
      resources:
        limits:
          cpu: 200m
          memory: 400Mi
        requests:
          cpu: 50m
          memory: 100Mi
      admissionWebhooks:
        patch:
          nodeSelector:
            taints.eks.aws.uk.tsb/role: mgmt
          tolerations:
            - key: role
              operator: Equal
              value: mgmt
              effect: NoSchedule

      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
      ##
      hostNetwork: false

    ## Component scraping the kube controller manager
    ##
    kubeControllerManager:
      enabled: false
    ## Component scraping kube scheduler
    ##
    kubeScheduler:
      enabled: false
    ## Component scraping kube proxy
    ##
    kubeProxy:
      enabled: false

    ## Deploy a Prometheus instance
    ##
    prometheus:
      enabled: true
      serviceAccount:
        create: true
        name: prometheus-sa
        annotations:
          eks.amazonaws.com/role-arn: arn:aws:iam::{{ AWS_ACCOUNT }}:role/eks/prometheus-thanos-s3
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: alb
          alb.ingress.kubernetes.io/scheme: internal
          alb.ingress.kubernetes.io/target-type: ip
          alb.ingress.kubernetes.io/inbound-cidrs: "{{ VPC_CIDR }},10.161.48.0/20,10.198.165.58/32"
          alb.ingress.kubernetes.io/success-codes: 200,301,302
          alb.ingress.kubernetes.io/backend-protocol: HTTP
          alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
          external-dns.alpha.kubernetes.io/hostname: prometheus.{{ DNS_ZONE }}
        hosts:
          - prometheus.{{ DNS_ZONE }}
        paths:
          - /*

      ## Settings affecting prometheusSpec
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      prometheusSpec:
        nodeSelector:
          taints.eks.aws.uk.tsb/role: mgmt
        tolerations:
          - key: role
            operator: Equal
            value: mgmt
            effect: NoSchedule
        image:
          repository: quay.io/prometheus/prometheus
          tag: v2.23.0
        containers:
          - name: prometheus
            env:
              - name: http_proxy
                value: http://eu-west-2.proxy.aws.uk.tsb:3128
              - name: https_proxy
                value: http://eu-west-2.proxy.aws.uk.tsb:3128
              - name: no_proxy
                value: localhost,127.0.0.1,169.254.169.254,10.0.0.0/8,172.20.0.0/16,.internal,.eu-west-2.elb.amazonaws.com,vault.service.consul,ssm.eu-west-2.amazonaws.com,ssmmessages.eu-west-2.amazonaws.com,ec2.eu-west-2.amazonaws.com,ec2messages.eu-west-2.amazonaws.com,s3.eu-west-2.amazonaws.com,autoscaling.eu-west-2.amazonaws.com,secretsmanager.eu-west-2.amazonaws.com,kms.eu-west-2.amazonaws.com,ecr.eu-west-2.amazonaws.com,.eks.amazonaws.com,.ecr.eu-west-2.amazonaws.com,.s3.eu-west-2.amazonaws.com,.cluster.local,.aws.uk.tsb
        podMetadata:
          annotations:
            iam.amazonaws.com/role: arn:aws:iam::{{ AWS_ACCOUNT }}:role/eks/prometheus-thanos-s3
        retention: 2h
        disableCompaction: false
        resources:
          requests:
            memory: 1Gi
        storageSpec:
          volumeClaimTemplate:
            spec:
              volumeName: prometheus-data #ENSURE PV created before running this ref: storage.yaml
              accessModes:
                - ReadWriteMany
              storageClassName: efs-sc
              resources:
                requests:
                  storage: 20Gi
              selector:
                matchLabels:
                  prometheus: "data"
        additionalScrapeConfigs:
          - job_name: prometheus
            static_configs:
              - targets:
                - localhost:9090
          - job_name: alertmanager
            static_configs:
              - targets:
                - prometheus-kube-prometheus-alertmanager.prometheus:9093
          - job_name: ec2-discover-node-exporter
            ec2_sd_configs:
              - region: eu-west-2
                port: 9100
            relabel_configs:
              - source_labels: [__meta_ec2_private_ip]
                target_label: privateip
              - source_labels: [__meta_ec2_instance_id]
                target_label: instanceid
              - source_labels: [__meta_ec2_tag_ManagedBy]
                target_label: managedby
                regex: rackspace
                action: keep
              - source_labels: [__meta_ec2_tag_Name]
                target_label: instance
              - source_labels: [__meta_ec2_tag_Environment]
                target_label: environment
              - source_labels: [__meta_ec2_availability_zone]
                target_label: availabilityzone
              - source_labels: [__meta_ec2_instance_state]
                target_label: status
              - source_labels: [__meta_ec2_instance_type]
                target_label: instancetype
              - source_labels: [__meta_ec2_tag_subsystem]
                target_label: subsystem
          - job_name: ec2-discover-hdf-jmx
            ec2_sd_configs:
              - region: eu-west-2
                port: 9110
            relabel_configs:
              - source_labels: [__meta_ec2_private_ip]
                target_label: privateip
              - source_labels: [__meta_ec2_instance_id]
                target_label: instanceid
              - source_labels: [__meta_ec2_tag_ManagedBy]
                target_label: managedby
                regex: rackspace
                action: keep
              - source_labels: [__meta_ec2_tag_Name]
                regex: .*hdf.*
                action: keep
              - source_labels: [__meta_ec2_tag_Name]
                action: replace
                target_label: instancename
              - source_labels: [__meta_ec2_tag_Environment]
                target_label: environment
              - source_labels: [__meta_ec2_availability_zone]
                target_label: availabilityzone
              - source_labels: [__meta_ec2_instance_state]
                target_label: status
              - source_labels: [__meta_ec2_instance_type]
                target_label: instancetype
          - job_name: ec2-discover-scylla-default
            ec2_sd_configs:
              - region: eu-west-2
                port: 9180
            relabel_configs:
              - source_labels: [__meta_ec2_private_ip]
                target_label: privateip
              - source_labels: [__meta_ec2_instance_id]
                target_label: instanceid
              - source_labels: [__meta_ec2_tag_ManagedBy]
                target_label: managedby
                regex: rackspace
                action: keep
              - source_labels: [__meta_ec2_tag_Name]
                regex: .*scylla.*
                action: keep
              - source_labels: [__meta_ec2_tag_Name]
                action: replace
                target_label: instancename
              - source_labels: [__meta_ec2_tag_Environment]
                target_label: environment
              - source_labels: [__meta_ec2_availability_zone]
                target_label: availabilityzone
              - source_labels: [__meta_ec2_instance_state]
                target_label: status
              - source_labels: [__meta_ec2_instance_type]
                target_label: instancetype
          - job_name: 'blackbox'
            metrics_path: /probe
            params:
              module: [http_2xx_cacert]
            static_configs:
              - targets:
                - https://grafana.{{ DNS_ZONE }}
                - https://prometheus.{{ DNS_ZONE }}
                - https://consul.{{ DNS_ZONE }}
                - https://vault.{{ DNS_ZONE }}
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: blackbox-exporter-prometheus-blackbox-exporter.prometheus:9115
          - job_name: 'blackbox_4xx'
            metrics_path: /probe
            params:
              module: [http_4xx_cacert]
            static_configs:
              - targets:
                - https://jenkins.{{ DNS_ZONE }}
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: blackbox-exporter-prometheus-blackbox-exporter.prometheus:9115
          - job_name: consul-exporter
            metrics_path: /metrics
            static_configs:
              - targets:
                - consul-exporter-prometheus-consul-exporter:9107
          - job_name: cloudwatch-exporter
            metrics_path: '/metrics'
            honor_labels: true
            static_configs:
            - targets:
              - cloudwatch-exporter-prometheus-cloudwatch-exporter:9106

        ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
        ## prometheus resource to be created with selectors based on values in the helm deployment,
        ## which will also match the servicemonitors created
        ##
        serviceMonitorSelectorNilUsesHelmValues: false
        ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
        ## prometheus resource to be created with selectors based on values in the helm deployment,
        ## which will also match the PrometheusRule resources created
        ##
        ruleSelectorNilUsesHelmValues: false
        #ruleSelector:
        #   matchLabels:
        #     app: kube-prometheus-stack
        #     release: prod

        ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
        ## This section is experimental, it may change significantly without deprecation notice in any release.
        ## This is experimental and may change significantly without backward compatibility in any release.
        ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#thanosspec
        ##
        thanos:
          image: quay.io/thanos/thanos:v0.17.2
          version: v0.17.2
          objectStorageConfig:
            name: thanos-s3-config
            key: object-store.yaml

    ## Configuration for alertmanager
    ##
    alertmanager:
      alertmanagerSpec:
        secrets: ['servicenow-secret']
        resources:
          requests:
            memory: 50Mi
        nodeSelector:
          taints.eks.aws.uk.tsb/role: mgmt
        tolerations:
          - key: role
            operator: Equal
            value: mgmt
            effect: NoSchedule
      config:
        global:
          resolve_timeout: 5m
        inhibit_rules:
          - source_match:
              severity: 'CRITICAL'
            target_match:
              severity: 'WARNING'
            equal: ['deployment']
          - source_match:
              severity: 'CRITICAL'
            target_match:
              severity: 'WARNING'
            equal: ['statefulset']
        receivers:
          - name: "null"
          - name: itom-dev-servicenow
            webhook_configs:
            - url: "https://tsbbankdev1.service-now.com/api/itsbb/alertmanager"
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
                basic_auth:
                  username: 'prometheus_user'
                  password_file: '/etc/alertmanager/secrets/servicenow-secret/servicenow_pw.txt'
          - name: itom-test-servicenow
            webhook_configs:
            - url: "https://tsbbanktest1.service-now.com/api/itsbb/alertmanager"
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
                basic_auth:
                  username: 'prometheus_user'
                  password_file: '/etc/alertmanager/secrets/servicenow-secret/servicenow_pw.txt'
          - name: itom-prod-servicenow            
            webhook_configs:
            - url: "https:// tsbprod001.service-now.com/api/itsbb/alertmanager"  
            send_resolved: true
            http_config:
              proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
              basic_auth:
                username: 'prometheus_user'
                password_file: '/etc/alertmanager/secrets/servicenow-secret/servicenow_pw.txt'
          - name: rax-monitoring-email
            webhook_configs:
            - url: "http://alertmanager-sns-forwarder-svc:9087/alert/{{ AWS_ENVIRONMENT }}-rax-alertmanager"
              send_resolved: true
          - name: itops-monitoring-email
            webhook_configs:
            - url: "http://alertmanager-sns-forwarder-svc:9087/alert/{{ AWS_ENVIRONMENT }}-itops-alertmanager"
              send_resolved: true
          - name: cmos-event-management
            webhook_configs:
            - url: "https://webhook.cmos-eventmanagement.com:9443/probe/webhook"
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
          - name: rax-monitoring-slack
            slack_configs:
            - channel: '#tsb-monitoring-alerts'
              api_url: 'https://hooks.slack.com/services/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'
              icon_url: 'https://github.com/prometheus/docs/raw/master/static/prometheus_logo.png'
              title: |-
                [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.instance }}
                {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                  {{" "}}(
                  {{- with .CommonLabels.Remove .GroupLabels.Names }}
                    {{- range $index, $label := .SortedPairs -}}
                      {{ if $index }}, {{ end }}
                      {{- $label.Name }}="{{ $label.Value -}}"
                    {{- end }}
                  {{- end -}}
                  )
                {{- end }}
              text: >-
                {{ range .Alerts -}}
                *Alert:* {{ .Annotations.summary }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                {{ end }}
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
          - name: "sns-forwarder-cdcalerts"
            webhook_configs:
            - url: "http://alertmanager-sns-forwarder-svc:9087/alert/tsb-CDCalerts"
              send_resolved: true
          - name: "sns-forwarder-cbcriticalalerts"
            webhook_configs:
            - url: "http://alertmanager-sns-forwarder-svc:9087/alert/cb-criticalalerts"
              send_resolved: true
          - name: 'cb-slack-notifications'
            slack_configs:
            - channel: '#cb_alert_notifications'
              api_url: 'https://hooks.slack.com/services/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'
              icon_url: 'https://github.com/prometheus/docs/raw/master/static/prometheus_logo.png'
              title: |-
                [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
                {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                  {{" "}}(
                  {{- with .CommonLabels.Remove .GroupLabels.Names }}
                    {{- range $index, $label := .SortedPairs -}}
                      {{ if $index }}, {{ end }}
                      {{- $label.Name }}="{{ $label.Value -}}"
                    {{- end }}
                  {{- end -}}
                  )
                {{- end }}
              text: >-
                {{ range .Alerts -}}
                *Alert:* {{ .Annotations.summary }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                *Description:* {{ .Annotations.description }}
                {{ end }}
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
          - name: 'cb-slack-notifications-new'
            slack_configs:
            - channel: '#cb_alerts_pre_prod'
              api_url: 'https://hooks.slack.com/services/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'
              icon_url: 'https://github.com/prometheus/docs/raw/master/static/prometheus_logo.png'
              title: |-
                [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
                {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                  {{" "}}(
                  {{- with .CommonLabels.Remove .GroupLabels.Names }}
                    {{- range $index, $label := .SortedPairs -}}
                      {{ if $index }}, {{ end }}
                      {{- $label.Name }}="{{ $label.Value -}}"
                    {{- end }}
                  {{- end -}}
                  )
                {{- end }}
              text: >-
                {{ range .Alerts -}}
                *Alert:* {{ .Annotations.summary }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                *Description:* {{ .Annotations.description }}
                {{ end }}
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
          - name: 'cb-critical-slack-notifications'
            slack_configs:
            - channel: '#cb_alerts_critical_prod'
              api_url: 'https://hooks.slack.com/services/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'
              icon_url: 'https://github.com/prometheus/docs/raw/master/static/prometheus_logo.png'
              title: |-
                [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
                {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                  {{" "}}(
                  {{- with .CommonLabels.Remove .GroupLabels.Names }}
                    {{- range $index, $label := .SortedPairs -}}
                      {{ if $index }}, {{ end }}
                      {{- $label.Name }}="{{ $label.Value -}}"
                    {{- end }}
                  {{- end -}}
                  )
                {{- end }}
              text: >-
                {{ range .Alerts -}}
                *Alert:* {{ .Annotations.summary }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                *Description:* {{ .Annotations.description }}
                {{ end }}
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"
          - name: 'cb-warning-slack-notifications'
            slack_configs:
            - channel: '#cb_alerts_warning_prod'
              api_url: 'https://hooks.slack.com/services/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'
              icon_url: 'https://github.com/prometheus/docs/raw/master/static/prometheus_logo.png'
              title: |-
                [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
                {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                  {{" "}}(
                  {{- with .CommonLabels.Remove .GroupLabels.Names }}
                    {{- range $index, $label := .SortedPairs -}}
                      {{ if $index }}, {{ end }}
                      {{- $label.Name }}="{{ $label.Value -}}"
                    {{- end }}
                  {{- end -}}
                  )
                {{- end }}
              text: >-
                {{ range .Alerts -}}
                *Alert:* {{ .Annotations.summary }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                *Description:* {{ .Annotations.description }}
                {{ end }}
              send_resolved: true
              http_config:
                proxy_url: "http://eu-west-2.proxy.aws.uk.tsb:3128"

        route:
          group_by: ['alertname']
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 30m
          receiver: "null"
          routes:
            #- match_re:
            #    namespace: cb
            #    group: INFRA
            #    environment: preprod|prod
            #  receiver: sns-forwarder-cbalerts
            #  continue: true
            - match:
                namespace: cb
                group: INFRA
                environment: prod
                severity: CRITICAL
              receiver: sns-forwarder-cbcriticalalerts
              continue: true
            - match_re:
                environment: preprod|prod
                application: cdc
              receiver: sns-forwarder-cdcalerts
              continue: true
            - match:
                namespace: cb
                group: INFRA
                environment: prod
              receiver: cb-slack-notifications
              continue: true
            - match_re:
                namespace: cb
                group: INFRA
                environment: preprod
                severity: WARNING|CRITICAL
              receiver: cb-slack-notifications-new
              continue: true
            - match:
                namespace: cb
                group: INFRA
                environment: prod
                severity: CRITICAL
              receiver: cb-critical-slack-notifications
              continue: true
            - match:
                namespace: cb
                group: INFRA
                environment: prod
                severity: WARNING
              receiver: cb-warning-slack-notifications
              continue: true
            - match_re:
                severity: CRITICAL
                rackspace: "true"
                environment: preprod|prod
              receiver: rax-monitoring-email
              continue: true
            - match_re:
                severity: CRITICAL
                rackspace: "true"
                environment: preprod|prod
              receiver: rax-monitoring-slack
              continue: true
            - match_re:
                severity: CRITICAL|WARNING
                environment: preprod|prod
              receiver: itom-dev-servicenow
              continue: true
            - match_re:
                severity: CRITICAL|WARNING
                environment: preprod|prod
              receiver: itom-test-servicenow
              continue: true
            - match_re:
                itopsbridge: "true"
                environment: prod
              receiver: itom-prod-servicenow
              continue: true
            - match_re:
                itopsbridge: "true"
                environment: prod  
              receiver: itops-monitoring-email
              continue: true
            - match_re:
                environment: preprod|prod
                application: cdc
              receiver: cmos-event-management
              continue: true
