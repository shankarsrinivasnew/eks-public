apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: kube-prometheus-stack
    release: prometheus
  name: scylla-monitoring
  namespace: prometheus
spec:
  groups:
    - name: scylla.rules
      rules:
        - record: cql:all_shardrate1m
          expr: sum(rate(scylla_cql_reads[60s])) by (cluster, dc, instance, shard) + sum(rate(scylla_cql_inserts[60s]) ) by (cluster, dc, instance, shard) + sum( rate(scylla_cql_updates[60s]) ) by (cluster, dc, instance, shard) + sum( rate(scylla_cql_deletes[60s])) by (cluster, dc, instance, shard)
        - record: cql:all_system_shardrate1m
          expr: sum(rate(scylla_cql_reads_per_ks{ks="system"}[60s])) by (cluster, dc, instance, shard) + sum(rate(scylla_cql_inserts_per_ks{ks="system"}[60s]) ) by (cluster, dc, instance, shard) + sum( rate(scylla_cql_updates_per_ks{ks="system"}[60s]) ) by (cluster, dc, instance, shard) + sum( rate(scylla_cql_deletes_per_ks{ks="system"}[60s])) by (cluster, dc, instance, shard)
        - record: cql:local_shardrate1m
          expr: sum(rate(scylla_storage_proxy_coordinator_reads_local_node[60s])) by (cluster, dc, instance, shard) + sum(rate(scylla_storage_proxy_coordinator_total_write_attempts_local_node[60s]) ) by (cluster, dc, instance, shard)
        - record: cql:all_rate1m
          expr: sum(cql:all_shardrate1m) by (cluster, dc, instance)
        - record: cql:non_token_aware
          expr: (sum(cql:all_rate1m) by (cluster) >bool 100) * clamp_min(1-(sum(cql:local_shardrate1m) by (cluster) / sum(cql:all_rate1m) by (cluster)), 0)
        - record: cql:non_system_prepared1m
          expr: clamp_min(sum(rate(scylla_query_processor_statements_prepared[1m])) by (cluster, dc, instance, shard) - cql:all_system_shardrate1m, 0)
        - record: cql:non_prepared
          expr: (sum(cql:non_system_prepared1m) by (cluster) >bool 100) * (sum(cql:non_system_prepared1m) by (cluster) / clamp_min(sum(cql:all_rate1m) - sum(cql:all_system_shardrate1m) by (cluster), 0.001))
        - record: cql:non_paged_no_system1m
          expr: clamp_min(sum(rate(scylla_cql_unpaged_select_queries[60s])) by (cluster, dc, instance) - sum(rate(scylla_cql_unpaged_select_queries_per_ks{ks="system"}[60s])) by (cluster, dc, instance), 0)
        - record: cql:non_paged_no_system
          expr: (sum(cql:non_paged_no_system1m) by (cluster, dc, instance) >bool 100) * sum(cql:non_paged_no_system) by (cluster, dc, instance)/clamp_min(sum(rate(scylla_cql_reads[60s]))by (cluster, dc, instance) - sum(rate(scylla_cql_unpaged_select_queries_per_ks{ks="system"}[60s])) by (cluster, dc, instance), 0.01)
        - record: cql:non_paged
          expr: (sum(cql:non_paged_no_system1m) by (cluster) >bool 100) * sum(cql:non_paged_no_system1m) by (cluster)/clamp_min(sum(rate(scylla_cql_reads[60s]))by (cluster) - sum(rate(scylla_cql_unpaged_select_queries_per_ks{ks="system"}[60s])) by (cluster), 0.01)
        - record: cql:reverse_queries
          expr: sum(rate(scylla_cql_reverse_queries[60s])) by (cluster)/ sum(rate(scylla_cql_reads[60s])) by (cluster)
        - record: cql:allow_filtering
          expr: sum(rate(scylla_cql_filtered_read_requests[60s])) by (cluster)/ sum(rate(scylla_cql_reads[60s])) by (cluster)
        - record: cql:any_queries
          expr: sum(rate(scylla_query_processor_queries{consistency_level="ANY"}[60s])) by (cluster) >bool 0
        - record: cql:all_queries
          expr: sum(rate(scylla_query_processor_queries{consistency_level="ALL"}[60s])) by (cluster) >bool 0
        - record: errors:operation_unavailable
          expr: sum(rate(scylla_storage_proxy_coordinator_read_unavailable[60s])) by (cluster, dc, instance) + sum(rate(scylla_storage_proxy_coordinator_write_unavailable[60s])) by (cluster, dc, instance) + sum(rate(scylla_storage_proxy_coordinator_range_unavailable[60s])) by (cluster, dc, instance)
        - record: errors:local_failed
          expr: sum(rate(scylla_storage_proxy_coordinator_read_errors_local_node[60s])) by (cluster, dc, instance) + sum(rate(scylla_storage_proxy_coordinator_write_errors_local_node[60s])) by (cluster, dc, instance)
        - record: errors:nodes_total
          expr: errors:local_failed + errors:operation_unavailable
        - record: manager:repair_done_ts
          expr: timestamp(sum(changes(scylla_manager_task_run_total{status="DONE",type="repair"}[60s])) by (cluster) > 0) or on(cluster) manager:repair_done_ts
        - record: manager:backup_done_ts
          expr: timestamp(sum(changes(scylla_manager_task_run_total{status="DONE",type="backup"}[60s])) by (cluster) > 0) or on(cluster) manager:backup_done_ts
        - record: manager:repair_fail_ts
          expr: timestamp(sum(changes(scylla_manager_task_run_total{status="ERROR",type="repair"}[60s])) by (cluster) > 0) or on(cluster) manager:repair_fail_ts
        - record: manager:backup_fail_ts
          expr: timestamp(sum(changes(scylla_manager_task_run_total{status="ERROR",type="backup"}[60s])) by (cluster) > 0) or on(cluster) manager:backup_fail_ts
        - record: manager:repair_progress
          expr: (max(scylla_manager_task_active_count{type="repair"}) by (cluster) >bool 0)*((max(scylla_manager_repair_token_ranges_total) by(cluster)<= 0)*0 or on(cluster) (sum(scylla_manager_repair_token_ranges_success>=0) by (cluster) + sum(scylla_manager_repair_token_ranges_error>=0) by (cluster))/sum(scylla_manager_repair_token_ranges_total>=0) by (cluster))
        - record: manager:backup_progress
          expr: (max(scylla_manager_task_active_count{type="backup"}) by (cluster) >bool 0)*((max(scylla_manager_backup_files_size_bytes) by(cluster)<= 0)*0 or on(cluster) (sum(scylla_manager_backup_files_uploaded_bytes) by (cluster) + sum(scylla_manager_backup_files_skipped_bytes) by (cluster) + sum(scylla_manager_backup_files_failed_bytes)by(cluster))/sum(scylla_manager_backup_files_size_bytes>=0) by (cluster))
        - record: wlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, dc, instance, shard, scheduling_group_name, le))
          labels:
            by: "instance,shard"
        - record: wlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, dc, instance, scheduling_group_name, le))
          labels:
            by: "instance"
        - record: wlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, dc, scheduling_group_name, le))
          labels:
            by: "dc"
        - record: wlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, scheduling_group_name, le))
          labels:
            by: "cluster"
        - record: rlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, dc, instance, shard, scheduling_group_name, le))
          labels:
            by: "instance,shard"
        - record: rlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, dc, instance, scheduling_group_name, le))
          labels:
            by: "instance"
        - record: rlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, dc, scheduling_group_name, le))
          labels:
            by: "dc"
        - record: rlatencyp99
          expr: histogram_quantile(0.99, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, scheduling_group_name, le))
          labels:
            by: "cluster"
        - record: wlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, dc, instance, shard, scheduling_group_name, le))
          labels:
            by: "instance,shard"
        - record: wlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, dc, instance, scheduling_group_name, le))
          labels:
            by: "instance"
        - record: wlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, dc, scheduling_group_name, le))
          labels:
            by: "dc"
        - record: wlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_write_latency_bucket{}[60s])) by (cluster, scheduling_group_name, le))
          labels:
            by: "cluster"
        - record: rlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, dc, instance, shard, scheduling_group_name, le))
          labels:
            by: "instance,shard"
        - record: rlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, dc, instance, scheduling_group_name, le))
          labels:
            by: "instance"
        - record: rlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, dc, scheduling_group_name, le))
          labels:
            by: "dc"
        - record: rlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_read_latency_bucket{}[60s])) by (cluster, le))
          labels:
            by: "cluster"
        - record: wlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_write_latency_sum{}[60s])) by (cluster, dc, instance, shard)/sum(rate(scylla_storage_proxy_coordinator_write_latency_count{}[60s])) by (cluster, dc, instance, scheduling_group_name, shard)
          labels:
            by: "instance,shard"
        - record: wlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_write_latency_sum{}[60s])) by (cluster, dc, instance)/sum(rate(scylla_storage_proxy_coordinator_write_latency_count{}[60s])) by (cluster, dc, scheduling_group_name, instance)
          labels:
            by: "instance"
        - record: wlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_write_latency_sum{}[60s])) by (cluster, dc)/sum(rate(scylla_storage_proxy_coordinator_write_latency_count{}[60s])) by (cluster, scheduling_group_name, dc)
          labels:
            by: "dc"
        - record: wlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_write_latency_sum{}[60s])) by (cluster)/sum(rate(scylla_storage_proxy_coordinator_write_latency_count{}[60s])) by (cluster, scheduling_group_name)
          labels:
            by: "cluster"
        - record: rlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_read_latency_sum{}[60s])) by (cluster, dc, instance, shard)/sum(rate(scylla_storage_proxy_coordinator_read_latency_count{}[60s])) by (cluster, dc, instance, shard, scheduling_group_name)
          labels:
            by: "instance,shard"
        - record: rlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_read_latency_sum{}[60s])) by (cluster, dc, instance)/sum(rate(scylla_storage_proxy_coordinator_read_latency_count{}[60s])) by (cluster, dc, instance, scheduling_group_name)
          labels:
            by: "instance"
        - record: rlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_read_latency_sum{}[60s])) by (cluster, dc)/sum(rate(scylla_storage_proxy_coordinator_read_latency_count{}[60s])) by (cluster, dc, scheduling_group_name)
          labels:
            by: "dc"
        - record: rlatencya
          expr: sum(rate(scylla_storage_proxy_coordinator_read_latency_sum{}[60s])) by (cluster)/sum(rate(scylla_storage_proxy_coordinator_read_latency_count{}[60s])) by (cluster, scheduling_group_name)
          labels:
            by: "cluster"
        - record: casrlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_read_latency_bucket{}[60s])) by (cluster, dc, instance, shard, le, scheduling_group_name))
          labels:
            by: "instance,shard"
        - record: casrlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_read_latency_bucket{}[60s])) by (cluster, dc, instance, le, scheduling_group_name))
          labels:
            by: "instance"
        - record: casrlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_read_latency_bucket{}[60s])) by (cluster, dc, le, scheduling_group_name))
          labels:
            by: "dc"
        - record: casrlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_read_latency_bucket{}[60s])) by (cluster, le, scheduling_group_name))
          labels:
            by: "cluster"
        - record: caswlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_write_latency_bucket{}[60s])) by (cluster, dc, instance, shard, le, scheduling_group_name))
          labels:
            by: "instance,shard"
        - record: caswlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_write_latency_bucket{}[60s])) by (cluster, dc, instance, le, scheduling_group_name))
          labels:
            by: "instance"
        - record: caswlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_write_latency_bucket{}[60s])) by (cluster, dc, le, scheduling_group_name))
          labels:
            by: "dc"
        - record: caswlatencyp95
          expr: histogram_quantile(0.95, sum(rate(scylla_storage_proxy_coordinator_cas_write_latency_bucket{}[60s])) by (cluster, le, scheduling_group_name))
          labels:
            by: "cluster"
    #alerts
        - alert: cqlNonPrepared
          # expr: cql:non_prepared > 0
          expr: cql:non_prepared > 5
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: cqlNonPrepared
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some of the cql queries are non-prepared please ensure the number of non prepared queries are low'
            summary: Instance {{ $labels.instance }} non prepared statments
        - alert: cql:non_paged_no_system
          expr: cql:non_paged > 1
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: non_paged_no_system
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some SELECT queries are non-paged'
            summary: Instance {{ $labels.instance }} non paged statments
        - alert: cqlNoTokenAware
          expr: cql:non_token_aware > 1
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: cqlNoTokenAware
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some queries are not token-aware'
            summary: Instance {{ $labels.instance }} non token aware statments
        - alert: cqlReverseOrder
          expr: cql:reverse_queries > 0
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: cqlReverseOrder
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some queries use reverse order'
            summary: Instance {{ $labels.instance }} reverse order queries
        - alert: cqlAllowFiltering
          expr: cql:allow_filtering > 0
          for: 30m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: cqlAllowFiltering
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some queries use ALLOW FILTERING'
            summary: Instance {{ $labels.instance }} Allow filtering queries
        - alert: cqlCLAny
          expr: cql:any_queries > 0
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: cqlCLAny
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some queries use Consistency Level: ANY'
            summary: Instance {{ $labels.instance }} non prepared statments
        - alert: cqlCLAll
          expr: cql:all_queries > 0
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: cqlCLAll
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Some queries use Consistency Level: ALL'
            summary: Instance {{ $labels.instance }} non prepared statments
        - alert: nonBalancedcqlTraffic
          expr: abs(rate(scylla_cql_updates{conditional="no"}[1m]) - scalar(avg(rate(scylla_cql_updates{conditional="no"}[1m]))))/scalar(stddev(rate(scylla_cql_updates{conditional="no"}[1m]))+100) > 2
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: nonBalancedcqlTraffic
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} CQL queries are not balanced among shards {{ $labels.instance }} shard {{ $labels.shard }}'
            summary: Instance {{ $labels.instance }} CQL queries are not balanced
        - alert: nodeLocalErrors
          expr: sum(errors:local_failed) by (cluster, instance) > 0
          for: 30m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: nodeLocalErrors
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} Some operation failed at the replica side'
            summary: Instance {{ $labels.instance }}  Replica side Level error
        - alert: nodeIOErrors
          expr: sum(rate(scylla_reactor_aio_errors[60s])) by (cluster, instance) > 0
          for: 10s
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: nodeIOErrors
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} IO Errors can indicate a node with a faulty disk {{ $labels.instance }}'
            summary: Instance {{ $labels.instance }} IO Disk Error
        - alert: nodeCLErrors
          expr: sum(errors:operation_unavailable) by (cluster) > 0
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: nodeCLErrors
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} Some operation failed due to consistency level'
            summary: Instance {{ $labels.instance }} Consistency Level error
        - alert: preparedCacheEviction
          expr: sum(rate(scylla_cql_prepared_cache_evictions[2m])) by (cluster) + sum(rate(scylla_cql_authorized_prepared_statements_cache_evictions[2m])) by (cluster) > 100
          for: 5m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: preparedCacheEviction
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} The prepared-statement cache is being continuously evicted, which could indicate a problem in your prepared-statement usage logic.'
            summary: Instance {{ $labels.instance }} Prepared cache eviction
        - alert: InstanceDown
          expr: up{job="scylla"} == 0
          for: 30s
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: InstanceDown
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has been down for more than 30 seconds.'
            summary: Instance {{ $labels.instance }} down
        - alert: InstanceDown
          expr: absent(scylla_transport_requests_served{job="ec2-discover-scylla-default", shard="0"})
          for: 5m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: InstanceDown
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} instance is shutting down.'
            summary: Instance {{ $labels.instance }} down
        - alert: InstanceDown
          expr: scylla_node_operation_mode > 3
          for: 5m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: InstanceDown
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} instance is shutting down.'
            summary: Instance {{ $labels.instance }} down
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/var/lib/scylla"} / node_filesystem_size_bytes{mountpoint="/var/lib/scylla"}
            * 100 < 35
          for: 15m
          labels:
            severity: WARNING
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: DiskFull
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has less than 35% free disk space.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/var/lib/scylla"} / node_filesystem_size_bytes{mountpoint="/var/lib/scylla"}
            * 100 < 25
          for: 15m
          labels:
            severity: WARNING
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: DiskFull
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has less than 25% free disk space.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/var/lib/scylla"} / node_filesystem_size_bytes{mountpoint="/var/lib/scylla"}
            * 100 < 15
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: DiskFull
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has less than 15% free disk space.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: DiskFull
          expr: node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}
            * 100 < 20
          for: 5m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: DiskFull
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has less than 20% free disk space on the root partition.'
            summary: Instance {{ $labels.instance }} low disk space
        - alert: NoCql
          # expr: scylla_manager_healthcheck_cql_status == -1
          expr: scylla_manager_healthcheck_cql_status == 2
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: NoCql
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} has denied CQL connection for more than 30 seconds.'
            summary: Instance {{ $labels.instance }} no CQL connection
        - alert: HighLatencies
          expr: wlatencyp95{by="instance"} > 100000
          for: 30m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: HighLatencies
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has 95% high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Write Latency
        - alert: HighLatencies
          expr: wlatencypa{by="instance"} >10000
          for: 30m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: HighLatencies
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has average high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Write Latency
        - alert: HighLatencies
          expr: rlatencyp95{by="instance"} > 100000
          for: 30m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: HighLatencies
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has 95% high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Read Latency
        - alert: HighLatencies
          expr: rlatencypa{by="instance"} >10000
          for: 30m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: HighLatencies
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: '{{ $labels.instance }} has average high latency for more than 5 minutes.'
            summary: Instance {{ $labels.instance }} High Read Latency
        - alert: BackupFailed
          expr: (sum(scylla_manager_task_run_total{type=~"backup", status="ERROR"}) or vector(0)) - (sum(scylla_manager_task_run_total{type=~"backup", status="ERROR"} offset 3m) or vector(0)) > 0
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: BackupFailed
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Backup failed'
            summary: Instance {{ $labels.instance }} Backup task failed
        - alert: RepairFailed
          expr: (sum(scylla_manager_task_run_total{type=~"repair", status="ERROR"}) or vector(0)) - (sum(scylla_manager_task_run_total{type=~"repair", status="ERROR"} offset 3m) or vector(0)) > 0
          for: 15m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: RepairFailed
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Repair failed'
            summary: Instance {{ $labels.instance }} Repair task failed
        - alert: restart
          expr: resets(scylla_gossip_heart_beat[1h])>0
          for: 5m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: restart
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'For {{ $labels.instance }} Node restarted'
            summary: Instance {{ $labels.instance }} restarted
        - alert: oomKill
          expr: changes(node_vmstat_oom_kill[1h])>0
          for: 10m
          labels:
            severity: CRITICAL
            environment: "{{ AWS_ENVIRONMENT }}"
            monitoring: oomKill
            rackspace: "true"
            hostname: "{{ $labels.instance }}"
            tsbci: AWS_ELZ
          annotations:
            description: 'OOM Kill on {{ $labels.instance }}'
            summary:  A process was terminated on Instance {{ $labels.instance }}
